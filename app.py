# -*- coding: utf-8 -*-
"""app.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1apy7GUwGKniLbCHORLMDEgr294hOcmc1
"""

!pip install streamlit ffmpeg-python librosa opencv-python-headless torchvision

app_code = '''
import streamlit as st
import librosa
import numpy as np
import torch
import torch.nn as nn
import torchvision.models as models
import torchvision.transforms as transforms
import cv2

class TransformerEmotionClassifier(nn.Module):
    def __init__(self, input_dim=552, hidden_dim=256, num_heads=4, num_classes=8):
        super().__init__()
        self.embedding = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.2)
        )
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=hidden_dim,
            nhead=num_heads,
            dim_feedforward=512,
            dropout=0.2,
            batch_first=True
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=2)
        self.classifier = nn.Sequential(
            nn.Linear(hidden_dim, 128),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(128, num_classes)
        )

    def forward(self, x):
        x = self.embedding(x)
        x = x.unsqueeze(1)
        x = self.transformer(x)
        x = x.squeeze(1)
        return self.classifier(x)

model = TransformerEmotionClassifier()
model.load_state_dict(torch.load("best_model.pt", map_location=torch.device('cpu')))
model.eval()

resnet = models.resnet18(pretrained=True)
resnet = torch.nn.Sequential(*list(resnet.children())[:-1])
resnet.eval()

transform = transforms.Compose([
    transforms.ToPILImage(),
    transforms.Resize((224, 224)),
    transforms.ToTensor()
])

emotion_map = {
    0: "angry", 1: "calm", 2: "disgust", 3: "fearful",
    4: "happy", 5: "neutral", 6: "sad", 7: "surprised"
}

def extract_mfcc(audio_path, n_mfcc=40):
    y, sr = librosa.load(audio_path, sr=None)
    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)
    return np.mean(mfcc.T, axis=0)

def extract_video_features(video_path, frame_skip=30):
    cap = cv2.VideoCapture(video_path)
    features = []
    i = 0
    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break
        if i % frame_skip == 0:
            try:
                img = transform(frame).unsqueeze(0)
                with torch.no_grad():
                    feat = resnet(img).squeeze().numpy()
                features.append(feat)
            except:
                pass
        i += 1
    cap.release()
    return np.mean(features, axis=0) if features else np.zeros(512)

st.title("ðŸŽ­ Real-Time Emotion Recognition")
st.write("Upload a `.wav` audio and `.mp4` video to predict the speaker's emotion.")

audio_file = st.file_uploader("Upload Audio File", type=["wav"])
video_file = st.file_uploader("Upload Video File", type=["mp4"])

if audio_file and video_file:
    with st.spinner("Processing..."):
        audio_path = "temp_audio.wav"
        video_path = "temp_video.mp4"
        with open(audio_path, "wb") as f:
            f.write(audio_file.read())
        with open(video_path, "wb") as f:
            f.write(video_file.read())

        mfcc_feat = extract_mfcc(audio_path)
        video_feat = extract_video_features(video_path)
        combined = np.concatenate([mfcc_feat, video_feat])
        input_tensor = torch.tensor(combined, dtype=torch.float32).unsqueeze(0)

        with torch.no_grad():
            pred = model(input_tensor).argmax(dim=1).item()

        st.success(f"**Predicted Emotion:** {emotion_map[pred]}")
'''

# Save the app to a file
with open("app.py", "w") as f:
    f.write(app_code)